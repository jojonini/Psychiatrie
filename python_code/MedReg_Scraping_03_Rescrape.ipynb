{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MedReg-Scraping 2020 V2\n",
    "Offenbar sind beim ersten Scraping über das Wochenende nicht alle Datensätze korrekt heruntergeladen worden. Ein Vergleich mit den Datensätzen vom ersten Scraping und dem erneuten Scraping mit der Inhaltssuche von MacOS hat ergeben, dass beim erneuten Scraping von verschiedenen Fachrichtungen deutlich weniger Datensätze vorhanden sind. \n",
    "\n",
    "Zudem ist wird durch die Scraping-Pause auch immer mindestens ein File ausgelassen. Dieser Fehler soll mithilfe des Protokolls und einer Funktion, der die fehlenden URL's nachlädt korrigiert werden, um sicher zu sein, alle vorhanden Datensätze per Ende 2019 bzw. dem 15. Januar runtergeladen zu haben. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=52282 Scraping paused\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=60341 Scraping paused\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=69204 Scraping paused\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=70759 Scraping paused\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=78807 Scraping paused\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=87079 Scraping paused\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=94705 Scraping paused\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=97614 Scraping paused\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=99316 Scraping paused\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=106692 Scraping paused\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "import time\n",
    "import csv\n",
    "from time import sleep\n",
    "\n",
    "url_stamm = \"https://www.medregom.admin.ch/DE/Detail/Detail?pid=\"\n",
    "output_path = \"/Users/master/Downloads/medreg_2020/\"\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n",
    "\n",
    "#Loop für das Erstellen und Abfragen der URL:\n",
    "for url_id in range(50000,115000):\n",
    "    url_req = url_stamm + str(url_id) \n",
    "    #erstellt die Abfrage-URL für die Datensätze im range(x,y)\n",
    "    try:\n",
    "        r = requests.get(url_req, headers = headers, timeout=20)\n",
    "        #timout von 5 auf 20 Sekunden erhöht\n",
    "\n",
    "        with open(output_path + \"medreg_\" + str(url_id) + \".html\", 'w', encoding = 'utf-8') as f:\n",
    "            f.write(r.text)\n",
    "            f.close()\n",
    "\n",
    "            protokoll = [[url_id, url_req, r.status_code]]\n",
    "            #neu wird die uld_id ins Protokoll geschrieben, um \n",
    "            #später besser die übersprungenen Webseiten auslesen zu können (Re-Scraping)\n",
    "\n",
    "        with open(output_path + \"scrape2020_115K_protokoll.csv\", 'a') as p:\n",
    "            writer = csv.writer(p, dialect = 'excel')\n",
    "            writer.writerows(protokoll)\n",
    "            \n",
    "        sleep(random.uniform(0, 1))\n",
    "        #diese Funktion wurde beim ersten Scraping deaktiviert, um die Abfrage möglichst kurz zu halten\n",
    "        \n",
    "        #mit dieser Funktion nimmt der Scraper den Dienst wieder auf, nachdem der Zugriff temporär gesperrt wird:\n",
    "    except: \n",
    "        r = requests.exceptions.ConnectionError\n",
    "        print(url_req, 'Scraping paused')\n",
    "        time.sleep(120)\n",
    "        \n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das erneute Scraping wurde in drei Schritten durchgeführt und dauerte vom 14.1. 01:29 bis 15.1. 16:15. Dabei wurden 114'784 Dateien herunter geladen. Mit einem Re-Scraping sollen jetzt noch die übersprungenen Dateien nachgeladen werden. \n",
    "\n",
    "Dafür wird folgender Code ausgeführt, der am 13. und 14.1. entwickelt wurde (Notebook *Scraping Create Re-Scrape Function*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(52281, 52283), (60340, 60342), (69203, 69205), (70758, 70760), (78806, 78808), (87078, 87080), (94704, 94706), (97613, 97615), (99315, 99317), (106691, 106693)]\n",
      "[3253, 9858, 17742, 17743, 23821, 28775, 35525, 42448, 49861, 52282, 60341, 69204, 70759, 78807, 87079, 94705, 97614, 99316, 106692]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "missed_range = []\n",
    "#missed_ids = []\n",
    "#nach dem ersten Durchgang deaktiviert um die Werte nicht zu überschreiben.\n",
    "\n",
    "with open('/Users/master/Downloads/medreg_scraping_protokoll/scrape2020_115K_protokoll.csv', newline='') as csvfile:\n",
    "    protokoll = csv.reader(csvfile, delimiter=',')\n",
    "    \n",
    "    last = int(next(protokoll)[0])\n",
    "    for row in protokoll:\n",
    "        data = int(row[0])\n",
    "        if data - last > 1:\n",
    "            missed_range.append((last,data))\n",
    "        last = data\n",
    "\n",
    "print(missed_range)\n",
    "\n",
    "\n",
    "for row in missed_range:\n",
    "    for i in range(row[0]+1,row[1]):\n",
    "        missed_ids.append(i)\n",
    "        \n",
    "print(missed_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da ich das Scraping in drei Stufen durchgeführt habe und dafür jetzt auch drei Protokoll-Dateien vorliegen, lasse ich den Code einfach dreimal durchlaufen und baue so die Liste mit den fehlenden ID's. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt lase ich die fehlenden ID's der Liste *missed_ids* mit einem leicht modifizierten Scrape-Code nochmals laufen und lade sie direkt in den Ordner mit den gescrapten Daten nach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3253,\n",
       " 9858,\n",
       " 17742,\n",
       " 17743,\n",
       " 23821,\n",
       " 28775,\n",
       " 35525,\n",
       " 42448,\n",
       " 49861,\n",
       " 52282,\n",
       " 60341,\n",
       " 69204,\n",
       " 70759,\n",
       " 78807,\n",
       " 87079,\n",
       " 94705,\n",
       " 97614,\n",
       " 99316,\n",
       " 106692]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=3253 200\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=9858 200\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=17742 200\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=17743 200\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=23821 200\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=28775 200\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=35525 200\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=42448 200\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=49861 200\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=52282 200\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=60341 200\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=69204 200\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=70759 200\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=78807 200\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=87079 200\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=94705 200\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=97614 200\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=99316 200\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=106692 200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "import time\n",
    "import csv\n",
    "from time import sleep\n",
    "\n",
    "url_stamm = \"https://www.medregom.admin.ch/DE/Detail/Detail?pid=\"\n",
    "output_path = \"/Users/master/Downloads/medreg_2020/\"\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n",
    "\n",
    "#Loop für das Erstellen und Abfragen der URL:\n",
    "for url_id in missed_ids:\n",
    "    url_req = url_stamm + str(url_id) \n",
    "    #erstellt die Abfrage-URL für die Datensätze im range(x,y)\n",
    "    try:\n",
    "        r = requests.get(url_req, headers = headers, timeout=20)\n",
    "        \n",
    "        with open(output_path + \"medreg_\" + str(url_id) + \".html\", 'w', encoding = 'utf-8') as f:\n",
    "            f.write(r.text)\n",
    "            f.close()\n",
    "\n",
    "            protokoll = [[url_id, url_req, r.status_code]]\n",
    "            print(url_req, r.status_code)\n",
    "\n",
    "        with open(output_path + \"rescrape2020_protokoll.csv\", 'a') as p:\n",
    "            writer = csv.writer(p, dialect = 'excel')\n",
    "            writer.writerows(protokoll)\n",
    "            \n",
    "        sleep(random.uniform(0, 1))\n",
    "        #diese Funktion wurde beim ersten Scraping deaktiviert, um die Abfrage möglichst kurz zu halten\n",
    "        \n",
    "        #mit dieser Funktion nimmt der Scraper den Dienst wieder auf, nachdem der Zugriff temporär gesperrt wird:\n",
    "    except: \n",
    "        r = requests.exceptions.ConnectionError\n",
    "        print(url_req, 'Scraping paused')\n",
    "        time.sleep(120)\n",
    "        \n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voilà, alle fehlenden Dateien wurden nachgeladen! 114799 Files im Ordner, also komplett.\n",
    "\n",
    "Noch ein Check der Fehlermeldungen in den Protokolldateien:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200    19234\n",
       "500      561\n",
       "Name: 500, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/master/Downloads/medreg_scraping_protokoll/scrape2020_20K_protokoll.csv')\n",
    "df.iloc[ : , 2 ].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200    26726\n",
       "500     3267\n",
       "Name: 200, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/master/Downloads/medreg_scraping_protokoll/scrape2020_50K_protokoll.csv')\n",
    "df.iloc[ : , 2 ].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200    62503\n",
       "500     2486\n",
       "Name: 200, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/master/Downloads/medreg_scraping_protokoll/scrape2020_115K_protokoll.csv')\n",
    "df.iloc[ : , 2 ].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, keine 400, 404-Codes. Wir haben also insgesamt:\n",
    "\n",
    "- 108463 Einträge zurückerhalten mit Code 200\n",
    "- 6314 mit Code 500\n",
    "\n",
    "Aber leider wissen wir beim Error 500 nicht genau, was die Ursache war. Das Problem ist ja, dass nicht alle ID's einen Eintrag haben. Ruft der Scraper eine solche Seite auf, kommt ein Error 500 zurück. Allerdings könnte der Fehler 500 auch andere Ursachen haben.\n",
    "\n",
    "Etwas beeunruhigend ist die Tatsache, dass eine Stichprobe der Files zwei unterschiedliche Dateien zeigte, die mit Code 500 registriert wurden. Dies soll im weiteren Notebook MedReg_Scraping 04 Data Check <MedReg_Scraping 04 Data Check> genauer untersucht werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
