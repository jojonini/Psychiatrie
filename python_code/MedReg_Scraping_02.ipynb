{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MedReg-Scraping 2020 V2\n",
    "Leider sind beim ersten Scraping über das Wochenende offenbar nicht alle Datensätze korrekt heruntergeladen worden. Ein Vergleich mit den Datensätzen vom ersten Scraping und dem erneuten Scraping mit der Inhaltssuche von MacOS hat ergeben, dass beim erneuten Scraping von verschiedenen Fachrichtungen deutlich weniger Datensätze vorhanden sind. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=23821 Scraping paused\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=28775 Scraping paused\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=35525 Scraping paused\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=42448 Scraping paused\n",
      "https://www.medregom.admin.ch/DE/Detail/Detail?pid=49861 Scraping paused\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "import time\n",
    "import csv\n",
    "from time import sleep\n",
    "\n",
    "url_stamm = \"https://www.medregom.admin.ch/DE/Detail/Detail?pid=\"\n",
    "output_path = \"/Users/master/Downloads/medreg_2020_10K/\"\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n",
    "\n",
    "#Loop für das Erstellen und Abfragen der URL:\n",
    "for url_id in range(20001,50000):\n",
    "    url_req = url_stamm + str(url_id) \n",
    "    #erstellt die Abfrage-URL für die Datensätze im range(x,y)\n",
    "    try:\n",
    "        r = requests.get(url_req, headers = headers, timeout=20)\n",
    "        #timout von 5 auf 20 Sekunden erhöht\n",
    "\n",
    "        with open(output_path + \"medreg_\" + str(url_id) + \".html\", 'w', encoding = 'utf-8') as f:\n",
    "            f.write(r.text)\n",
    "            f.close()\n",
    "\n",
    "            protokoll = [[url_id, url_req, r.status_code]]\n",
    "\n",
    "        with open(output_path + \"scrape2020_50K_protokoll.csv\", 'a') as p:\n",
    "            writer = csv.writer(p, dialect = 'excel')\n",
    "            writer.writerows(protokoll)\n",
    "            \n",
    "        sleep(random.uniform(0, 1))\n",
    "        #diese Funktion wurde beim ersten Scraping deaktiviert, um die Abfrage möglichst kurz zu halten\n",
    "        \n",
    "        #mit dieser Funktion nimmt der Scraper den Dienst wieder auf, nachdem der Zugriff temporär gesperrt wird:\n",
    "    except: \n",
    "        r = requests.exceptions.ConnectionError\n",
    "        print(url_req, 'Scraping paused')\n",
    "        time.sleep(120)\n",
    "        \n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
